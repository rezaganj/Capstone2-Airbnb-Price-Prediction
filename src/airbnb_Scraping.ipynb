{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function scrapes the fees and home descriptions from Airbnb given url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping(df,db):\n",
    "    for i, url in enumerate(df.listing_url):\n",
    "        day = df.minimum_nights[i]+1\n",
    "        if db.count_documents({'link':url})==0:\n",
    "            try:\n",
    "                if day<10:\n",
    "                    new_url = url + f'?source_impression_id=p3_1616194878_hGlc%2FRxTeggNQPIp&check_in=2021-07-01&guests=1&adults=1&check_out=2021-07-0{day}'\n",
    "                else:\n",
    "                    new_url = url + f'?source_impression_id=p3_1616194878_hGlc%2FRxTeggNQPIp&check_in=2021-07-01&guests=1&adults=1&check_out=2021-07-{day}'\n",
    "                    \n",
    "                driver = webdriver.Firefox()\n",
    "                driver.get(new_url)\n",
    "                time.sleep(6)\n",
    "                htmlSource = driver.page_source\n",
    "                soup = BeautifulSoup(htmlSource,'html.parser')\n",
    "            \n",
    "                cleanliness = 0\n",
    "                Accuracy = 0\n",
    "                Communication = 0\n",
    "                Location = 0\n",
    "                Checkin = 0\n",
    "                Value = 0\n",
    "                Final_score = 0\n",
    "                Description = 'None'\n",
    "            \n",
    "                Description = soup.find_all('div','_1d784e5')[0].text \n",
    "                cleanliness = soup.find_all('div',\"_a3qxec\")[0].find('span','_4oybiu').text\n",
    "                Accuracy = soup.find_all('div',\"_a3qxec\")[1].find('span','_4oybiu').text\n",
    "                Communication = soup.find_all('div',\"_a3qxec\")[2].find('span','_4oybiu').text\n",
    "                Location = soup.find_all('div',\"_a3qxec\")[3].find('span','_4oybiu').text\n",
    "                Checkin = soup.find_all('div',\"_a3qxec\")[4].find('span','_4oybiu').text\n",
    "                Value = soup.find_all('div',\"_a3qxec\")[5].find('span','_4oybiu').text\n",
    "                Final_score = soup.find('div',\"_544wcx\").find('a','_5twioja').text\n",
    "                \n",
    "            \n",
    "                cleaning = soup.find('div',\"_ud8a1c\").find_all('span','_ra05uc')[1].text\n",
    "                service = soup.find('div',\"_ud8a1c\").find_all('span','_ra05uc')[2].text\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "            \n",
    "                cleaning = np.nan\n",
    "                service = np.nan\n",
    "             \n",
    "            record_to_add = {'link':url,'new_link':new_url,'cleaning_fee':cleaning,'service_fee':service, 'cleanliness':cleanliness , 'Accuracy':Accuracy , 'Communication':Communication , 'Location': Location , 'Checkin' : Checkin , 'Value' : Value , 'Final_score':Final_score , 'Description':Description}\n",
    "            db.insert_one(record_to_add)\n",
    "            driver.close()\n",
    "            sleep(1)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/listings_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['airbnb']\n",
    "comments = db['airbnb_scrape']\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the airbnb scores and descriptions\n",
    "web_scraping(df,comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the scraped data into a dataframe\n",
    "scraped_data = pd.DataFrame(list(comments.find({})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the scraped data into a csv file\n",
    "scraped_data.to_csv('Airbnb_Description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case to remove the database\n",
    "# client.drop_database('airbnb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
